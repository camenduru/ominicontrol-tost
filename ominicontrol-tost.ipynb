{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install aria2 -qqy\n",
    "!pip install transformers diffusers accelerate peft opencv-python protobuf sentencepiece optimum-quanto\n",
    "\n",
    "%cd /content\n",
    "!git clone https://github.com/Yuanshi9815/OminiControl\n",
    "%cd /content/OminiControl\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/scheduler/scheduler_config.json -d /content/model/scheduler -o scheduler_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/text_encoder/config.json -d /content/model/text_encoder -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/text_encoder/model.safetensors -d /content/model/text_encoder -o model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/text_encoder_2/config.json -d /content/model/text_encoder_2 -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/text_encoder_2/model-00001-of-00002.safetensors -d /content/model/text_encoder_2 -o model-00001-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/text_encoder_2/model-00002-of-00002.safetensors -d /content/model/text_encoder_2 -o model-00002-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/text_encoder_2/model.safetensors.index.json -d /content/model/text_encoder_2 -o model.safetensors.index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/tokenizer/merges.txt -d /content/model/tokenizer -o merges.txt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/tokenizer/special_tokens_map.json -d /content/model/tokenizer -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/tokenizer/tokenizer_config.json -d /content/model/tokenizer -o tokenizer_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/tokenizer/vocab.json -d /content/model/tokenizer -o vocab.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/tokenizer_2/special_tokens_map.json -d /content/model/tokenizer_2 -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/tokenizer_2/spiece.model -d /content/model/tokenizer_2 -o spiece.model\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/tokenizer_2/tokenizer.json -d /content/model/tokenizer_2 -o tokenizer.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/tokenizer_2/tokenizer_config.json -d /content/model/tokenizer_2 -o tokenizer_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/transformer/config.json -d /content/model/transformer -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/transformer/diffusion_pytorch_model-00001-of-00003.safetensors -d /content/model/transformer -o diffusion_pytorch_model-00001-of-00003.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/transformer/diffusion_pytorch_model-00002-of-00003.safetensors -d /content/model/transformer -o diffusion_pytorch_model-00002-of-00003.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/transformer/diffusion_pytorch_model-00003-of-00003.safetensors -d /content/model/transformer -o diffusion_pytorch_model-00003-of-00003.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/transformer/diffusion_pytorch_model.safetensors.index.json -d /content/model/transformer -o diffusion_pytorch_model.safetensors.index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/vae/config.json -d /content/model/vae -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/vae/diffusion_pytorch_model.safetensors -d /content/model/vae -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/raw/main/model_index.json -d /content/model -o model_index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Yuanshi/OminiControl/resolve/main/omini/subject_512.safetensors -d /content/lora -o subject_512.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/OminiControl\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from optimum.quanto import freeze, qfloat8, quantize\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n",
    "\n",
    "from src.condition import Condition\n",
    "from src.generate import generate\n",
    "\n",
    "with torch.inference_mode():\n",
    "    dtype = torch.bfloat16\n",
    "    # model = \"/content/model\"\n",
    "    model = \"black-forest-labs/FLUX.1-schnell\"\n",
    "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model, subfolder=\"text_encoder\", torch_dtype=dtype)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model, subfolder=\"tokenizer\", torch_dtype=dtype)\n",
    "    text_encoder_2 = T5EncoderModel.from_pretrained(model, subfolder=\"text_encoder_2\", torch_dtype=dtype)\n",
    "    tokenizer_2 = T5TokenizerFast.from_pretrained(model, subfolder=\"tokenizer_2\", torch_dtype=dtype)\n",
    "    vae = AutoencoderKL.from_pretrained(model, subfolder=\"vae\", torch_dtype=dtype)\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(model, subfolder=\"transformer\", torch_dtype=dtype)\n",
    "    quantize(transformer, weights=qfloat8, activations=qfloat8)\n",
    "    freeze(transformer)\n",
    "    pipe = FluxPipeline(scheduler=scheduler, text_encoder=text_encoder, tokenizer=tokenizer, text_encoder_2=text_encoder_2, tokenizer_2=tokenizer_2, vae=vae, transformer=None)\n",
    "    pipe.transformer = transformer\n",
    "    pipe.load_lora_weights(\"/content/lora\", weight_name=f\"subject_512.safetensors\", adapter_name=\"subject\",)\n",
    "    pipe.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    input_image = \"/content/image (2).webp\"\n",
    "    prompt = \"A very close up view of this item. It is placed on a wooden table. The background is a dark room, the TV is on, and the screen is showing a cooking show. With text on the screen that reads 'Omini Control!'\"\n",
    "    image = Image.open(input_image)\n",
    "    w, h, min_size = image.size[0], image.size[1], min(image.size)\n",
    "    image = image.crop(((w - min_size) // 2, (h - min_size) // 2, (w + min_size) // 2, (h + min_size) // 2,))\n",
    "    image = image.resize((512, 512))\n",
    "    condition = Condition(\"subject\", image)\n",
    "    output_image = generate(pipe, prompt=prompt.strip(), conditions=[condition], num_inference_steps=8, height=512, width=512).images[0]\n",
    "output_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OminiControl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
